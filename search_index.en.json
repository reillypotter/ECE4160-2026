[{"url":"https://reillypotter.github.io/ECE4160-2026/","title":"About Me","description":null,"body":"\n  Lucca Correia\n  \n    \n      \n    \n  \n  \n    \n      \n    \n  \n\n\n\n\n\n \nJunior at Cornell University studying Mechanical Engineering with a focus in Robotics\nHere on campus I am on the Cornell Nexus Project Team as well as the Cornell United Soccer Club Team. I will be updating my site through the semester to reflect my progress in Fast Robots - MAE4190/ECE4160.\n","path":null},{"url":"https://correial.github.io/LuccaFastRobots/Fast Robots Stuff/","title":"MAE 4190 Fast Robots Labs","description":null,"body":"","path":null},{"url":"https://correial.github.io/LuccaFastRobots/Fast Robots Stuff/lab-12/","title":"Lab 12: Path Planning and Execution","description":null,"body":"Overall\nI worked with Trevor and Jack to navigate our robot through a preset route of waypoints assigned to us in the lab (shown below). Overall, we were very successful in arriving and navigating through these waypoints completely autonomously with a combination of on-board PID control and off-board localization processing! The final result was incredibly rewarding as it required us to combine and implement various aspects of code and hardware debugging learned throughout the semester in addition to navigation and waypoint logic.\nWe began with complementary arrays of waypoints and localization booleans (i.e. a point, and a boolean to signal the script to localize at that waypoint). This allowed us to manually assign which points we localized at based on the accuracy of localization at each waypoint for the most accurate navigation. To navigate between points, we used sequential orientation and position PID using the DMP onboard our IMU, and Kalman-filtered TOF sensor, respectively.\nWe used my robot for this lab, and collaborated jointly on both Arduino and Python code components!\n\nNavigation Plan\nPython Implementation\nPython Code\nBelow is the core logic and control flow for our entire lab. It is responsible for calling all functions and Arduino commands. The python code is explained in length below and all Arduino commands are covered in the next section.\n\nCore Logic and Control Flow\nThe navigation path is defined by a list of 2D waypoints waypoint_list, with each point representing a target location in feet. An accompanying list of boolean flags localize_flags determines whether the robot should re-localize its position at each waypoint using a grid-based localization system.\nThe script continuously loops through all waypoints in sequence, executing the following steps for each:\n\n\nTurn Toward the Next Waypoint:\n\nCalculates the angle between the robot’s current position and the next waypoint using the atan2 function.\nSends a PID control command over Bluetooth to rotate the robot to the calculated heading (CMD.PID_TURN_CONTROL , described in artemis section).\n\n\n\nRead Current TOF Sensor Data:\nRequests the current pose from the robot (CMD.SEND_CURRENT_POSE, described in artemis section).\n\nExtracts the distance reading from the CSV.\n\n\n\nMove Toward the Waypoint:\n\nComputes the expected distance to the next waypoint based on current position belief.\nSubtracts the distance to the next point from the current TOF reading to calculate the necessary target distance for the next motion.\nSends a PID control command over Bluetooth to move the robot forward to the calculated target. (CMD.PID_CONTROL , described in artemis section)\n\n\n\nUpdate Robot Location:\n\nIf localization is enabled for the current waypoint, the robot turns to zero degrees and then performs an observation scan (function perform_observation_loop() from Lab 11) and sends the data through BLE. Then, an update step is performed to estimate the robot’s current position.\nIf localization is disabled, the robot’s position is assumed to be exactly at the waypoint.\n\n\n\nArduino Implementation\nNote that all data sent over from the Artemis to the computer over BLE is processed and piped into a CSV file by a notification handler. This CSV file is reset at specific points throughout the code before new ToF and Gyro data comes in.\nCMD.PID_TURN_CONTROL\nThis case is responsible for using orientation PID control to arrive at a desired target angle.\n\nInitially, an arrived boolean flag (triggered when the robot arrived within a threshold of the target angle) was used to end the PID control. However, this meant that if we slightly overshot the target, the flag would still say that we arrived as we did pass the target. At this point, the robot has overshot but the arrived flag is true and the PID control will no longer run. We tried to severly overdampen the system to fix this, however it left us with too much steady state error. Our solution was implementing the 5 second cutoff timer before stopping the PID; this gave us plenty of time to approach angles and eliminated the issue of early PID stoppage. The getDMP() and RunPIDRot variables are explain in great detail in lab 6. The clearVariables() function is seen in all cases used in this lab; it clears all varaibles in the entire arduino script at the end of cases so the next case is ready to run without any memory overflow.\nCMD.SEND_CURRENT_POSE\nThis case is responsible for sending a ToF reading to the computer over BLE to help inform the distance needed to travel.\n\ncollectTOFis explained in many previous labs; it is responsible for collecting a TOF value and appending it to the TOF1 array.\nCMD.PID_CONTROL\nThis case has a similar purpose to CMD.PID_TURN_CONTROL, except it is responsible for linear PID control rather than rotational.\n\nrunPIDLin() is the same function used in Lab 7 for integration of Kalman filtering onto the robot; it is responsible for running the linear PID control and is explained in Lab 7 and 5. Just as in orientation PID control, a cutoff timer is used to eliminate the early ending of PID control that occurs with am arrived flag.\nCMD.SPIN\nThis command is called in the perform_observation_loop() function and is responsible for localization gyro and ToF data collection; it is explained in lab 11. The only change is the swapping out of an arrived flag for a timer just as for orientation and linear PID cases.\nResults\nBelow are a series of trials from our lab! In the last two runs you can see a live belief map and logging in jupyter notebook which ouputs critical information such as if we are localizing at that specific waypoint, our current belief pose, our calculated target heading, etc. To try and speed up our runs, we did not localize at every point since our waypoint setting algorithm and PID control was fairly good at getting us between shorter distances.\nEarly Run\n\nEarly Run\nWe saw much success with the structure of our high-level planning on the first run, but ran into trouble with inconsistencies in lower level control and angle targeting. When we localized, our planner output was correct based on position belief, but we struggled to tune the orientation PID loop in such a way that it responded (similarly) well to changes in angle between 10 and 180 degrees, while settling within a reasonable time. We changed our approach to add a time cutoff to both control loops, sacrificing a bit of accuracy for longer-term operation.\nNearly There\n\nAlmost Perfect\nDuring this run we had to nudge robot at (-5,-3) due to a ToF underestimation. Also, the derivative term blew up at (5,3) while localizing so we had to manually face it at the temporary target angle. Also apologies as we dont scroll down on the jupyter lab script until later in the video so some of the outputs are hidden.\nFinal Run!\n\nFinal Run\nIt was awesome to see everything from the semester really come together perfectly during this run! You can see the localization do a great job of correcting for ToF underestimating and overall noise; coordinate (5,3) is a great example of this where it is short of the marker but accounts for it by angling slightly upward to approach the next waypoint where it then arrives with complete precision.\nCollaboration\nThank you to all the course TAs for all of the time and support during this entire semester! I worked completely with Jack Long and Trevor Dales. ChatGPT was used for checking specifics of implementing math functions in python.\n","path":null},{"url":"https://correial.github.io/LuccaFastRobots/Fast Robots Stuff/lab-11/","title":"Lab 11: Localization on the Real Robot","description":null,"body":"Simulation\nI started by verifying that the lab11_sim.ipynb file successfully produces the same localization results as from lab 10. From the image below, you can see that we obtain the expected outcome. Again, you can see the unreliability of the odometry model…\n\nLocalization Simulation\n\nBlue: belief pose\nRed: odometry model\nGreen: ground truth\n\nRobot Implementation\nPython Code\nUntil next lab we do not have a way of determening ground truth for the robot; for this lab only the update step of the Bayes filter is implemented via the perform_observation_loop() function shown below. Its purpose is to perform the observation loop behavior on the real robot and return two column numpy arrays: (1) ToF range values (meters) and (2) sensor bearings in degrees.\nIn the function you can see that the SPIN case is called over BLE. It is responsible for conducting data collection and is further discussed in the Arudino Code section below. After the data is sent over BLE post collection, it is piped into a CSV file via the notification handler and passed into the localization algorithm by parsing through the CSV.\n\nArduino Code\nA good deal of code and groundwork was reused from my lab 9 in order to have the robot spin on axis and collect DMP and ToF data. Angular PID was used via the DMP to reliably arrive at each target angle. The main difference is that for localization, I wanted to ensure that each measurment was as accurate as possible and that only a set number of ToF measurments were being taken. To achieve this, the ToF measruments were taken only when the robot arrived at each angle. After arriving at each angle, the robot waits for 500 ms to ensure a steady measurment, then the new target angle is set and the robot moves again.\nAt first, I tried to take a total of 18 measurments (20° between each). However, this did not provde to be enough measurments for a reliable estimate of pose, thus I doubled it to 36 measurments (with 10° between each). Note that I had to update the observations_count: 36 line in the world.yaml file.\n\nRefer to lab 9 for a detailed breakdown of each function and sub function being called.\nResults\nI localized about four different coordinates within the world. The true position of the robot is the green dot; the belief based on localization is the blue dot. From the four locations below, you can see that the localization is fairly good at getting close to the exact location of the robot, but is not completely accurate. From analyzing polar plots and ToF data, it appears that the ToF tends to underestimate which accounts for the deviation from the ground truth. For example, if you look at (-3,-2), you can see that the robot believes its closer to the corner than it really is due to this underestimation.\nFor lab 12 I to continue troubleshooting the ToFs. One idea is to fuse measurments from the two ToF sensors on the robot to reduce the effect of noise.\n\nLocalization @ (0,3)\n\nLocalization @ (5,-3)\n\nLocalization @ (-3,-2)\n\nLocalization @ (5,3)\nThe following video shows the update step being completed at (5,-3)\n\nLocalization @ (5,-3) Vid\nDiscussion\nA good deal of debugging and trouble shooting was required to make the localization work. The change of 18 ToF to 36 ToF measurments was already discussed above. I also had to angle my ToF sensor up more to reduce ground interference. Furthermore, I played around with the sensor noise found in world.yaml and ended up reducing it from sensor_sigma: 0.1 to sensor_sigma: 0.05.\nCollaboration\nI collaborated extensively on this project with Jack Long and Trevor Dales. ChatGPT was used to help answer Bayes Filter implementation questions.\n","path":null},{"url":"https://correial.github.io/LuccaFastRobots/Fast Robots Stuff/lab-10/","title":"Lab 10: Grid Localization using Bayes Filter","description":null,"body":"Introduction\nOur robot lack absolute or “ground truth” knowledge of its position. As a result, it must infer its location using information from the environment (ToF sensors)—a process known as localization, which is most effective when approached probabilistically. To implement probabilistic localization, we use a Bayes filter, which maintains a “belief” about the robot’s position. Based on its initial state as well as ToF sensor data and control inputs, the robot forms an estimate of where it might be. As new sensor readings and inputs are received, the robot continuously updates this belief using Bayesian inference. This lab aims to implement Bayes Filter via simulation before implementing on the robot.\nThe robot state is 3 dimensional and is given by (x, y, $ \\theta $). The robot’s world is a continuous space that spans with dimensions:\n\n-5.5 ft &lt; x &lt; 6.5 ft in the x direction\n-4.5 ft &lt; x &lt; 4.5 ft in the y direction\n-180°, +180° along the theta axis\n\nThe robot’s environment is represented by a grid with cells measuring 0.3048 m x 0.3048 m x 20° for a total number of cells along each axis being (12,9,18). Each cell stores the probability of the robot being in that position, with all probabilities summing to 1 to represent the robot’s belief. The Bayes filter updates these probabilities over time, and the cell with the highest probability at each step indicates the robot’s most likely pose, forming its estimated trajectory.\nBayes Filter\nThe Bayes filter is a iterative process that begins by estimating the robot’s next position using control inputs, and then refines this estimate based on sensor measurements. Here is the model skeleton:\n\n\n\n\n\n\n\nAlgorithm Bayes_Filter ($bel(x_{t-1}), \\ u_t, \\ z_t$)\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for all $x_t$ do\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\overline{bel}(x_t) = \\sum_{x_{t-1}} p(x_t \\mid u_t, x_{t-1}) \\ bel(x_{t-1})$\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$bel(x_t) = \\eta \\ p(z_t \\mid x_t) \\ \\overline{bel}(x_t)$\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;end for\nreturn $bel(x_t)$\n\nThe prediction model can be broken down into two main parts:\n\nPrediction Step: the robot estimates its current position based on its previous pose and control inputs\nUpdate Step: this is the correction step of the Bayes Filter, where the predicted belief and sensor liklihoods are incorperated to decreases the uncertainty in the robot’s position.\n\nOdometry Motion Model\nFor our purpose, control input u is expressed via an Odometry Motion Model which describes the difference between two positions or “poses”. This difference is composed of three parameters: an initial rotation, a translation, and a final rotation.\nAlgorithm Implementation\nThe following functions were used as the logic and implementation for the Bayes filter\nCompute Control\nThe compute_control() function takes two paramenters: current pose and a previous pose. From the parameters it determines the initial rotation, translation, and final rotation which are sufficient to reconstruct the relative motion between two robot states.\n\nOdometry Model Parameters\nBelow are the equations and code used to calculate and return the rotations and translation\n\n$\\delta_{rot1} = \\text{atan2}(\\bar{y}’ - \\bar{y}, \\bar{x}’ - \\bar{x}) - \\bar{\\theta}$\n$\\delta_{trans} = \\sqrt{(\\bar{y}’ - \\bar{y})^2 + (\\bar{x}’ - \\bar{x})^2}$\n$\\delta_{rot2} = \\bar{\\theta}’ - \\bar{\\theta} - \\delta_{rot1}$\n\n\nOdometry Motion Model\nThe odom_motion_model() function takes in a current pose and a previous pose and is responsible for  predicting the probability of where a robot might be after applying noisy motion commands. Extracting the current pose, previous pose, and actual vs. predicted control inputs via compute_control(), it  calculates the probabilities density of those parameters as Gaussian distributions passing in the standard deviations for rotation and translation noise.\n\nPrediction Step\nThe prediction_step() function estimates the robot’s predicted belief by applying the odometry motion model to account for movement uncertainty. It takes the current and previous odometry readings, extracts control parameters, and iterates over all possible previous and current poses within the discretized 3D state space (x, y, θ). For each possible transition, it calculates the probability of moving from a previous pose to a current pose using odom_motion_model(). These probabilities are accumulated to form a predicted belief (bel_bar), which is then normalized to maintain a valid probability distribution.\nTo improve computational efficiency, prediction_step() skips previous states with extremely low probabilities (below 0.0001) since they contribute negligibly to the final belief. This trade-off sacrifices a small amount of accuracy and completeness in favor of significantly faster execution.\n\nSensor Model\nThe robot’s sensors are assumed to have Gaussian noise in their readings. The sensor_model() function, like the odometry motion model, calculates the likelihood of data but based on sensor measurements instead of movement. It essentially takes a set of sensor observations at a given pose and returns the probability of receiving those observations.\n\nUpdate Step\nFinally, the update_step() function loops through the current state grid, calls the sensor_model() to obtain sensor likelihoods, and updates the belief accordingly. The updated belief is then normalized to ensure it sums to 1.\nThe below equation expresses how to calculate the total likelihood of a full set of sensor measurements zₜ given a robot’s pose xₜ and map m.\n$$\np(z_t \\mid x_t, m) = \\prod_{k=1}^{18} p(z_t^k \\mid x_t, m)\n$$\n\nSimulation\nThe video below shows the shows a trajectory and localization simulation using the Bayes filter. The trajectory is pre-planned to move around the box. Here, the odometry model is plotted in red while the ground truth tracked by the simulator is plotted in green. From the video you can see the inaccuracy of the odometry model when operating in isolation.\nThe probabilistic belief is plotted in blue which you can see is a very good approximation of the robot’s true position. The probability distribution is also shown using the white boxes where stronger shades of white represent stronger probabilities. Note that we ignored all cells with probability &lt;0.0001.\n\nBayes Simulation\nNotice that the initial estimation before a new sensor value is based on the odemetry model which makes sense. I also found that The Bayes Filter seems to perform better when the robot is near walls. This is likely due to the sensors being more accurate and consistent at sensing closer distances, and therfore more trustworthy. On the other hand, when it is in more open spaces such as the center of the map, the filter is slightly less accurate.\nCollaboration\nI collaborated extensively on this project with Jack Long and Trevor Dales. I referenced Stephan Wagner’s site for help with implementing the Bayes Filter. ChatGPT was used to help answer Bayes Filter implementation questions and conceptual misunderstandings.\n","path":null},{"url":"https://correial.github.io/LuccaFastRobots/Fast Robots Stuff/lab-9/","title":"Lab 9: Mapping","description":null,"body":"Orientation Control Implementation\nOrientation control via the IMU’s DMP library was used to set target angles for ToF data collection. With success and high accuracy using the DMP in Lab 6, I figured that this was the best approach to ensure that the robot was reliably turning a set number of degrees throughout its rotation.\nBLE and Data Transmission\nThe SPIN case handled initiating and running the turning logic as well as sending data over BLE at the end of collection. The case was called over bluetooth via the command: ble.send_command(CMD.SPIN, \"4.6|0|0\"). The parameters are used to pass through PID gains for more efficient tuning and are ordered as follows: Kp|Ki|Kd. I found that with the small changes in target angles, and the goal of moving fairly slow, that only proportional control was needed to reliably come within a degree or two of the target. At the slow speeds, there was minimal, if any, overshoot thus a derivative term was not needed. A notification handler is used to recieve the data and help pipe it into CSV files as used in previous labs.\nArduino Implementation\nThe SPIN code is shown below. The robot spins a full 360 degrees; this is broken up into 12 degrees per movement every two seconds for a total of  30 rotations. The next target angle (adding 12 to target_turn) is set every two seconds to allow for ample data collection time at a fixed angle for maximum ToF accuracy. Note that I have have ToF sensors collecting data at all times to increase the number of data points. While this does come at the cost of decreased accuracy while rotating between each two second break, it did not seem to greatly impact the ToF readings and the increased resultion was well worth the slightly decreased accuracy at a few points. Roughly 2780 ToF readings were collected per full rotation (not all necessarily unique).\n\nThere are a few important functions to note, many of which have been used in my previous labs:\n\ngetDMP() is responsible for updating the global yaw array with the current angle reading\ncollectTOF() updates the global ToF arrays with the current ToF readings regardless if whether a new one is ready\nsendPIDTURNData() is responsible for sending over all relevant arrays over BLE\nrunPIDRot() (shown below) is responsible for handling all rotational PID logic–it passes in the gains, target, and current angle into the pid_turn_Gyro() function which is explained in detail in Lab 6\n\n\nOn-Axis Rotation Testing\nDrift Error\nI set up a 1ft by 1ft square with a ruler in frame to test the drift of the robot over the course of the 360 degree scan. From the video you can see that the drift throughout mapping was roughly three inches likely due to wheel slip, ground friction despite wheel tape, and an inbalance of motor power. In a 4×4 meter room, this drift would not accumulate fully in one direction, but rather we can asume that it averages out across the scan point. As a result, the expected average positional error in the final map would be roughly 1.5 inches (3.8 cm). This results in a worst-case error under 1%, which is acceptable for an empty 4×4 m room. While I tried to use correction terms to reduce the drift, I was not able to completely mitigate it.\n\n360° On-Axis Test\nData for Target Setting\nNote you can see that for roughly 2/3 of rotations, there was no overshoot as a reverse correction input was not needed (seen on the below graph). For instances of small overshoots, they were quickly corrected as seen from a spike in speed in the opposite direction. If I wanted to completely eliminate these overshoots, I would have to make the rotations slower. This results in a memory overflow issue. The 12 degree timing every two seconds comes very close to maxing out the dynamic memory on the Artemis.\n\nRotational PID and Target Setting\nMapping Data and Post Processing\n360 Scan at Global Coordinates\nFor all runs, the robot was started at zero degrees facing the same way as shown in the video. The video shows a full ToF sweep taken at the coordinate (0,3)\n\n360° Data Collection Scan\nThe data at each coordinate was then graphed on a polar plot for an initial sanity check\n\nMapping Scans\nTransformation Matrices\n\nRobot Coordinate System\nA few transformations were used to convert the raw ToF data into the global arena coordinate system. I started by taking into account the front ToF sensor location relative to the robot’s center via the robot’s coordinate axis (shown above). I found that the ToF sensor (“TOF” in the matrix) is positioned 70 mm $\\hat{x}$, 0 mm $\\hat{y}$ from the robots center.\nThe following position vector is yielded\n\n\nNext, a transformation is needed to convert the robot’s local angular yaw coordinate (from the DMP) to global $\\hat{x}$ and $\\hat{y}$ coordinates\n\n\nMap\nAfter the transformations are applied to the raw ToF and DMP data, the following map is created\n\nToF Arena Map\nThe blue arc in the middle of the arena likely indicates that a new ToF reading was not recieved for a few degrees of rotation, essentially creating part of a circle as the same ToF value is being plotted for a small sweep of angles. Additionally straight lines in the middle of the map are likely due to sensor noise as they appear in every trial.\nLine-Based Map Conversion\nI estimated the arena walls from the ToF map and overlayed it onto the graph. Note that some of the estimated walls appear slanted. This may be the result of a block being slightly angled at the time of data collection. The dotted black line represents the estimated arena from the ToF values\n\nToF Arena Map with Estimated Lines and True Map\nThe following start and end arrays were used to create the estimated arena map (dotted black line above)\n\nTrue Map\n\nI also added the true map of the arena (in purple) knowing that every tile is 1ft x 1ft\n\n\n  \n    \n    ToF Arena Map with Estimated Lines and True Map\n  \n  \n    \n    ToF Arena Map with Estimated Lines and True Map\n  \n\nYou can see that the fit between the estimated and true map are fairly close; the largest deviations appear to be at the center box and protruding wall at th bottom center. Again, I would say that this is mostly the result of ToF noise as it is constantly changing the distance away from the wall that it is seeing. Noise is especially evident when the robot is pointed at relatively far walls.\nCollaboration\nI collaborated extensively on this project with Jack Long and Trevor Dales. ChatGPT was used to help plot and format graphs.\n","path":null},{"url":"https://correial.github.io/LuccaFastRobots/Fast Robots Stuff/lab-8/","title":"Lab 8: Stunts","description":null,"body":"\nCars 3 Way Tie\nFlip Implementation\nBluetooth Data Transmission\nThe entire flip program starts when called over bluetooth via the Jupyter notebook line: ble.send_command(CMD.STUNT, \"1200\"). The parameter passed in is the distance away from the wall that the car should begin to reverse at. Note that 1200 mm is larger than the distance away from the wall of the mat (300 mm) as the car needs time to slow down before ultimately flipping on the mat. This parameter allows for easy flip tuning for various different starting positions and flip locations.\nArduino Code\nThe STUNT command is responsible for ensuring the first kalman filter value is stored and  calling the essential Flip() and sendStuntData() functions. Here is the command:\n\nThe functions to note are:\n\nFlip()runs the logic to drive at the wall then flip the robot. While the robot has yet to reach the flip distance, it drives forward at full speed. Once it reaches the target distance, it runs in reverse at full speed for 2300 ms. Note that during both loops, ToF and Kalman data is being collected.\n\n\n\ncollectTOF() responsible for collecting current ToF values and updating the global TOF1 array with distances\ngetKalmanData() reads in the current TOF1 array value (updated by the collectTOF() function). In return it updates the global kf array with the kalman filter calculated distance which is used for tracking distance to the wall for the stunt. In the below code, you can see the use of an update flag that flips based on whether new sensor data is ready– this helps to speed up the process. The following A and B matrices were used post tuning:\n\n\n\n\n\n\nsendStuntData() loops through crucial arrays and sends them over bluetooth to be processed by the notification hanlder and piped into a csv for graphing and post prosessing.\n\nResults\nStunt Videos\nThe time elapsed for each stunt trial are in the image captions; the timer starts once the car passes the blue line and ends once it retuns and crosses it.\n\nFlip Trial 1 | Time: 2.38s\n\nFlip Trial 2 | Time: 2.36s\n\nFlip Trial 3 | Time: 2.30s\nGraphs for Trial 3\nFrom the graph you can see Kalman Filter Position vs. Time as well as Speed vs. Time. Note that the speed flips from 255 to -255 once the robots hits the target distance of 1200 mm. This distance was played around with until finally deciding that 1200 mm was just enough to have the robot flip on the mat (300 mm from the wall) without hitting the wall or flipping too early. You can see that the Kalman filter could use some further tuning as the model predicts too slow of a velocity (as seen from the lack of steepness in the bunched up data). Returning to the state space model, this could be improved by reducing m (to increase the B matrix) or increasing U (less intuitive fix). ToF sensor data is visible from the KF jumps but aren’t shown to reduce graph clutter.\n\nTrial 3 Graph\nBlooper Video\nHere is my blooper video! (Here is the original but vote on the edited cars version)\n\nBlooper\nSummary and Challenges\n\nYou can see that in two of the trials, an added weight is mounted to the front of the robot to help it nosedive and flip about its front. By the end I realized that the added mass did not help as much as a fully charged battery thus you can see that the trials with the added weight (1 and 2) are nearly identical to those without. The weight was made out of taped together washers and was shared between Trevor and I.\n\n\nRobot with (right) and without (left) weight\n\nThroughout the hours of testing, I had to use correction factors to straighted the car’s trajectory toward the mat and also help to slow the wheels down at the same time, ensuring that when flipped, the robot was oriented straight. My correction terms scale the passed in PWM speed. When driving at the wall, a correction term of 0.95 scales the right side motor and when driving in reverse (slowing down) a correction factor of 0.90 scales the left side motor. The blooper is an example of the robot before tuning. You can see it arc left when approaching the wall and then spinning as it slowed down which made it ultimately return at the completely wrong angle.\n\nCollaboration\nI collaborated extensively on this project with Jack Long and Trevor Dales. ChatGPT was used to help plot graphs.\n","path":null},{"url":"https://correial.github.io/LuccaFastRobots/Fast Robots Stuff/lab-7/","title":"Lab 7: Kalman Filter","description":null,"body":"Drag and Mass Estimations\nWe can start with approximating the open-loop robot system as first-order using Newton’s second law\n$$ F = ma = m\\ddot{x} $$\nthen adding the linear drag force (d) and motor input (u),\n$$ F = -d\\dot{x} + u $$\nthe dynamics of the system can be described in terms of second derivative of position\n$$ \\ddot{x} = -\\frac{d}{m} \\dot{x} + \\frac{u}{m} $$\nThen, we can represent the system in state space notation with the state vector\n\n\nThe corresponding dynamics in the state space form \n\n\n\nFor this system, the drag and mass can be estimated as the1 following lumped parameters covered in lecture\n$$ d = \\frac{u_{ss}}{\\dot{x}_{ss}} $$\n$$\nm = \\frac{-d \\cdot t_{0.9}}{\\ln(1 - 0.9)}\n$$\nI drove the car at the wall with a PWM value of 120 (in the range of values used for Lab 5) to collect data needed to estimate the above variables. Below are the graphs showing the ToF data and corresponding velocity for the drive at the wall. The velocity was calculated by taking the difference in ToF values over the difference in time for each new ToF data. Data was piped into a CSV for later use via a notification handler with the same structure as used in Lab 5.\n\nDrive at Wall\n\nToF and Velocity Data\nFrom the steady-state velocity (2.459 m/s), 90% rise time (1.21 s) and speed at 90% rise time (2.213 m/s) printed in the upper right of the graph, as well as setting U to 1 N (unit step) we can now calculate m and d:\n$$\nd = \\frac{1\\ \\mathrm{N}}{2.459\\ \\mathrm{m/s}} \\approx 0.407\\ \\mathrm{kg/s}\n$$\n$$\nm = \\frac{0.407\\ \\mathrm{kg/s} \\cdot 2.213\\ \\mathrm{m/s}}{\\ln(1 - 0.9)} \\approx .391\\ \\mathrm{kg}\n$$\nFinally, we can define our state space matrices in terms of known quantities:\n\n\n\nSince we are only directly measuring ToF data X, the state space which will give us that output are the following C and D matrices:\n\n\n\nKalman Python Simulation\nInitialize Python KF\nFor the initial python simulation, the sampling time used for the Kalman filter was 95 ms, the same as my ToF rate. When I confirmed that the simulation was working, I then sped it up to the rate that my PID code loop runs on my arduino (20 ms). U_ss was calculated via u/step_size, where u = 120 and step size = 255. I estimated my ToF variance (dx = 30mm) by taking the average variance in data when statically measuring distance from the wall.\nThe below code was used to discretize my A and B matrices and define my C and state vector\n\nNext, I specified my initial process noise and sensor noise covariance matrices via the following equations from lecture.\n\n\n\n\n\nThe following code from lecture was used for my Kalman filter\n\nTesting KF Simulation (ToF Speed)\nI started with testing the simulation using the variables calculated in the initialization KF section. You can see that the Kalman fit is pretty good– it tracks the ToF well but isn’t so close that it completely disregards the position and velocity from the kalman state dynamics when a new ToF comes in.\n\ndt = 0.095 | dx = 0.47 | $\\sigma_1$ &amp; $\\sigma_2$ = 32.4 | $\\sigma_3$ = 57.5\nJust for a test, I tried to have my Kalman filter follow my ToF more closely. I achieved this by increasing the $\\sigma_1$ and $\\sigma_2$ terms which effectively put more trust in the ToF data values. Here is the result of doubling $\\sigma_1$ and $\\sigma_2$ terms. You can see how closely the Kalman filter tracks the ToF data.\n\ndt = 0.095 | dx = 0.47 | $\\sigma_1$ &amp; $\\sigma_2$ = 65 | $\\sigma_3$ = 57.5\nFor a final simulation test I wanted to check the other extreme: what happens if I effectively only trust the model? I did this by increasing $\\sigma_3$ by an order of 1,000,000 times larger than $\\sigma_1$ and $\\sigma_2$. You can see in the below graph that the Kalman filter is effectively only using the state space dynamics to predict the next distance value rather than ever meshing with the ToF data.\n\ndt = 0.095 | dx = 0.47 | $\\sigma_1$ &amp; $\\sigma_2$ = 32.4 | $\\sigma_3$ = 1,000,000\nTesting KF Simulation (PID Speed)\nI sped up my dt term to the PID loop speed of 20 ms to simulate how it will run at the faster PID loop speed when integrated onto the robot.\n\ndt = 0.02 | $\\sigma_1$ &amp; $\\sigma_2$ = 32.4 | $\\sigma_3$ = 57.5\nAgain, I felt as though my my Kalman values were tracking too close to my ToF, thus I increased $\\sigma_3$ to 170 (3x the original value) and left $\\sigma_1$ and $\\sigma_2$ constant\n\n$\\sigma_1$ &amp; $\\sigma_2$ = 32.4 | $\\sigma_3$ = 170\nOnboard Robot Kalman Integration\nInitialize Arduino KF\nAfter confirming that the simulation worked as expected, I integrated the code onto my car. The general workflow is as follows:\nThe below code runs in the main loop without any delays. The pid_start flag is sent over a bluetooth command along with the PID gains. runPIDLin() is responsible for running the Kalman filter and passing the outputs into the PID logic function. After the data arrays are filled they are sent to the computer over artemis by the sendPIDData() function.\n\nBelow is the runPIDLin() function. We can break down each function called\n\ncollectTOF() collects the next ToF value and is responsible for setting the update variable to true dependings on if the ToF value is new or not. The ToF value (new or old) is added to the global TOF1 array.\nkalman() is responsible for running the Kalman filter. Note that the runPIDLin() functions runs as fast as possible thus the kalman filter is especially useful in predicting a distance measurment when new ToF values are not ready. I found that I had to scale my input speed value by 1000 for a better kalman response with from the state space dynamics (this is discussed again at the end).\npid_speed_ToF() feeds the distance value from the kalman filter into the PID loop which ultimately sends a speed to the motors. Note that the pid_speed_ToF function is effectively the same as from Lab 5.\n\n\nHere is the kalman() function. Note that the inverse and transpose functions had to be swapped out for Arduino alternatives\n\nTest Arduino\nThere was extensive debugging required to implement the Kalman filter onto the robot. I will be showing three test cases in this section to summarize all of the robot integration work and results.\n\nI started by testing the Kalman filter state dynamics similar to how the kalman dynamics model was tested in the python simulation. I did this by effectively putting zero trust in my ToF data relative to my dynamics model by making $\\sigma_3$  an order of 1,000,000 times larger than $\\sigma_1$ and $\\sigma_2$. This yielded the following graph which proves that the Kalman filter is standalone and independent– it properly follows the system dynamics derived at the beginning of the lab when not fusing the Kalman predicted values with ToF.\n\n\nKalman Filter Dynamics Test\n\nI implemented proportional control and ran the robot at a wall with the target of stopping 300 mm (~1ft) away. With only proportional control, I was unable to avoid hitting the wall unless the robot went extremely slow. Here is a trial where the robot does hit the wall but is able to bounce off and still meet the 1 ft target. Note that the speed is constant (horizontal) during some of the run due to the speed floor that is implemented. You can also see the speed ceiling in the PD control at the beginning.\n\n\nKalman Filter Proportional Control Kp = .1\n\nKalman Filter Poportional Control\n\nThe PD Control worked really well. Relative to the proportional control above, you can see that the rise time does slightly increase as the derivative term is helping to slow the car down earlier, but it allows for effectively zero overshoot. If I continued to increase the proportional and derivative gains, I might have been able to achieve a similarly quick rise time with the derivative term still reducing overshoot. The robot stopped right about 1 ft away from the wall with little steady state error; as in previous labs, an integral term did not seem to be needed. You can see the implementation of a low pass filter to reduce derivative kick and overall noise spikes. From the graph you can see that the Kalman filter follows the ToF data fairly well and, as proved above, is able to make its own predicitions using the state dynamcis which is especially useful when new ToF data isn’t ready.\n\n\nKalman Filter PD Kp = .12 | Kd = 60\n\nKalman Filter PD Control\nVariable Discussion\nQuick recap of variables used to tune and their impacts:\n\nm and d: Although these parameters were derived from experimentally determined variables at the start, they can still be fine-tuned. A higher value of m corresponds to a car with greater inertia; it accelerates less in response to a control input. The d term mainly influences the max speed of the car (calculated by the Kalman filter) and how rapidly the speed decreases once the control input is removed.\nu: The control input, unlike the m and d terms, come directly from the PID controller and in theory probably should not be changed in order to tune the kalman filter. However, I found that scaling/ampliyfing it (as mentioned earlier in the lab) was a quick and effective workaround to better match the filter’s expected behavior.\n$\\boldsymbol{\\sigma}$: As previously mentioned in the lab, $\\sigma_1$ and $\\sigma_2$ represent the “trust” in the ToF data realitve to the model. For example, large $\\sigma_1$ and $\\sigma_2$ correspond to low confidence in the model’s predictions and make the filter rely more heavily on sensor measurments. $\\sigma_3$ is effectively the same but for the model. Large $\\sigma_3$ values represent more trust in the model than in the ToF measurments.\n\nCollaboration\nI collaborated extensively on this project with Jack Long and Trevor Dales. I referenced Stephan Wagner’s site with help in implementing the python kalman simulation! ChatGPT was used to help plot graphs.\n","path":null},{"url":"https://correial.github.io/LuccaFastRobots/Fast Robots Stuff/lab-6/","title":"Lab 6: Orientation Control","description":null,"body":"Bluetooth Communication and PID/DMP Code\nPID code is handeled and sent over bluetooth in a very similar fasion to lab 5!\nMain Function\nHere is the turning PID part of the main function. The PID turning code waits until the start_time_pid_turn flag is set to 1 via the PID_TURN_CONTROL command that is responsible for sending the Kp, Ki, and Kd gains as well as sending the starting flag: ble.send_command(CMD.PID_TURN_CONTROL, \"1.5|0|40|90\") # P|I|D. The getDMP and runPIDROT() functions are covered in detail in the next sections. After the turning PID arrays are filled, the robot is then instructed to stop and the sendPIDTURNData() function is used to loop through all arrays and send them over to be piped into a CVS via my notification hander.\nMain Loop\n\nNotification Handler\n\ngetDMP() Function\nThe getDMP function is responsible for updating the global yaw_gy varaible whenever there is a new value, and if there is not (pretty rare) the previous value is used to avoid storing zeros in the yaw array. Also, it is worth noting that if we were deriving the yaw angle by taking the integral of the gyroscope’s angular velocity over time it would not make sense to then take the derivative. However, since the DMP is providing discrete angle readings, we do need to take these derivatives still.\n\nrunPIDRot() Function\nThe runPIDRot() function is responsible for calling the PID turning function (pid_turn_Gyro()) and ensuring that it has the current distance fed in as a parameter. The function also populates the PID arrays\n\npid_turn_Gyro() Function\nThere are three key aspects of the PID turning control loop:\n\nWhen I first was testing with proportional and PD control, I noticed that what was really happening was that the speed was just jumping between the floor value on the negative and positve side as seen in the below image. To account for this I decided that I needed to map the calculated speeds (in the range of 0-255) into the range scaled to my robot’s floor and ceiling. This was done using the Arduino map command speedTurn_set_mapped = map(speedTurn_set, 0, 255, minSpeedTurn, maxSpeedTurn). The result is that my robot is now using real PID control (which will be shown later) rather than just going between the PWM floor.\n\n\nNo Mapping Control\n\n\nA low pass filter was used on the derivative term to reduce noise and the effect of derivative kick. The alpha value was calculated through a slight trial and error method where I found a balance between reducing noise (from the filtered_d_term) but also reducing speed/amplitude as fast as possible when needing to slow down (from the current d_term). The alpha that optimized this trade off was 0.1.\n\n\nWhile testing for hours I noticed that regardless of my gains (I tried Kd terms on the order of 1000), my robot would not be able to stop in time and over shot its target. I realized that it was not a coding issue but rather the cars inability to change directions fast enough. I also noticed that the overshoot only happened on the first approach– if I picked the car up and replaced it at the beginning location after it had been running for a while the car was able to stop perfectly. To account for this, I have the car delay for 1 second at a low PWM before starting. This solved the issue completely!\n\n\n\nPID Control and Gains Discussion\nPID Gains\nAfter much testing, I decided that the best final gains were Kp=1.65 and Kd=130. The Kp term is high enough to adjust and continue moving even when at small angles from the target. At the same time, it is not so big that it overshoots a large angle from the target. The Kd term is responsible for slowing down the car as it approaches the target angle. It is large enough to help slow the car down and reduce overshoot. With hours of tuning, I realized that if I continued to increase the Kd term, there would be more instability as I osccilate quickly about the target distance. The progression of choosing and testing gains is shown later. I found that my robot was able to arrive within a degree or two of the target distance and thus without the presence of considerable external noise, I did not see the need to implement integral control as well.\nTesting\nDisturbance Correction (PD)\nHere are my two final disturbance tests. The first is with Kp=1.8 and Kd=50. You can see that the car slightly overshoots but is able to come within a few degrees of the 90 degree target each time it restabalizes.\n\nDisturbance Correction Kp=1.8 | Kd=50\nHere is the other final disturbance test with Kp=1.65 and Kd=130. You can see that for these gains, the robot overshoots less but is just a little worst at re-aligning exactly at 90 degrees. Thus while it overshoots less, it is slower when moving to a set angle (with the increaed Kd gain that helps it slow down earlier).\n\nDisturbance Correction Kp=1.65 | Kd=130\nHere is the plotted data after being sent over bluetooth:\n\nDisturbance Correction Kp=1.65 | Kd=130\nYou can certainly see some derivative kick with the increased Kd term, however, the LPF does take care of it for the most part.\nTurning Way Points\nTo start thinking about future applications of the PID controller I added waypoints that the robot would turn to. During navigation this can be implemented along side of the straightline PID controller; the robot doesn’t need to turn in an arc while it drives, instead it can drive straight, pivot, then drive straight again. In this example, the robot turns from 0 degrees to 90 degrees, then back to 0 degrees, then ends at 120 degrees:\n\nWaypoints\nHere is the graph for my waypoint setting:\n\nWaypoint Data\nYou can certianly see some more derivative kick, but again, it is filtered out well by the LPF.\nSampling Time Discussion\nThe IMU polling rate is set to 1.1 kHz in the line: success &amp;= (myICM.enableDMPSensor(INV_ICM20948_SENSOR_GAME_ROTATION_VECTOR) == ICM_20948_Stat_Ok);. The sampling time for our system is the DMP output rate ODR which is set in my setup function: success &amp;= (myICM.setDMPODRrate(DMP_ODR_Reg_Quat6, 2) == ICM_20948_Stat_Ok);. The 2 sets the ODR to an overall 549 Hz, plenty fast for the PID turning loop.\n$$ ODR = \\frac{\\text{DMP running rate}}{\\text{ODR setting}} - 1 $$\nPolling rate is 1.1 kHz (1100 Hz)\n$$ ODR = \\frac{1100}{2} - 1 = 550 - 1 = 549 \\text{ Hz} $$\nCollaboration\nI collaborated extensively on this project with Jack Long and Trevor Dales. I referenced Stephan Wagner’s site for much of the lab but most specifically for implementing the DMP! ChatGPT was used to help plot graphs.\n","path":null},{"url":"https://correial.github.io/LuccaFastRobots/Fast Robots Stuff/lab-5/","title":"Lab 5: Linear PID control and Linear interpolation","description":null,"body":"Bluetooth Communication and PID Code\nData communication to and from the computer and Artemis were handeled similar to previous labs. The PID.CONTROL command is used to send the start flag and the Kp, Ki, Kd, and target distance: ble.send_command(CMD.PID_CONTROL, \".05|0|6|300\") # P|I|D. The Arduino recieves the gains:\n\nThe main loop is responsible for running the runPID() function which collects the ToF data and internally runs the pid_speed_ToF() responsible for calculating the speed based on PID control feedback. The calculated speed is then passed into the forward() and reverse() functions from Lab 4. The function is run until the PID data array is filled (500 in this case).\n\nBelow is the int pid_speed_ToF() function. Note that after calculating the PID speed is calculated, there are various IF statments to account for the motor deadband, direction, stopping tolerance, and max speed. Effectively, the function logic flows as follows:\n\nCalculate PID Speed from gains and error\nIf the speed is &lt;1 it means we are effectively at our target and can stop\nIf we are not yet close enough to the target we check to ensure our set speed is lower than the max speed (in either direction). If it isn’t we bring the speed down to maxSpeed\nIf the speed falls within our deadband range from Lab 4 (PWM of 35), the PWM signal is brought up to 35\nThe set speed is passed into the reverse() or forward() function based on its sign\n\n\nAfter the data is collected, it is sent back to the computer over Bluetooth via the sendPIDData() function. On my computer the data is piped into a CSV file via my notification handler. From there, the data is plotted for visualization.\nPID Control and Tuning\nRange/Sampling Time Discussion\nThe following code was implemented to ensure that new ToF values were returned at least every 35ms. This is vital in PID control as we want to make sure we are re-calculating our speed frequently as possible even if it means trading off some accuracy. If new values are not recieved fast enough, the robot risks running into the wall or not being able to react fast enough to sudden changes in surroundings.\n\nProportional Control\nFor this lab the car starts roughly 2m-4m from the wall and is intended to stop 300mm from the wall. As a result, ToF readings (in mm) start at 2000-4000. For this lab, the intention is to keep the car speed to no more than ~150. If we focus on the very start of running the car, the error is roughly 3000-300 = 2700. Thus, my propotional gain values (which get multiplied by the 2700mm error) were in the range of .05 - .08 throughout tuning to keep us in the 100-150 PWM when at these large distances from the wall at the start. When the error gets very small, the Kp term is not large enough to provide the minimum PWM to move the car. As a result, the deadband minimum speed was implemented.\nBelow are two videos of only proportional control implementation with the car starting at ~2500mm from the wall. The first is with Kp = 0.5 and the second with Kp = 0.8. You can see that with the increased proportional gain, the car was unable to stop which is why a Kd term is added in the next section:\n\nProportional Control #1\n\nProportional Control #1\nPD Control\nWhile soley propotional control proved to be fairly accurate (effectively stopping &lt;4mm from the target), overshoot and ramming into the walls was still an issue at higher speeds. Thus, since I wanted to try increasing my speed I have to implement a derivative term. The derivative term helps to slow the car down proportional to the slope of the error. As the car approaches the wall the error is decreasing resulting in a negative derivative term that helps reduce the overall speed. You can see that with the same Kp = .08 value that previously crashed the car we are now able to stop early enough to avoid crashing with the addition of a derivative gain.\nI ran the robot with the following gains Kp = .08 &amp; Kd = 6\n\nPD Control\n\nPD Control\nDiscrete Kd values can be seen as the ToF values are unfotunely slow. When there is not a new ToF reading, the derivative term is zero as the current and previous error terms are the same until a new ToF reading comes in. To address this issue we extrapolate. Effectively, the derivative term is only slowing the car down at discrete intervals which is not enough to slow the car down in time at much higher speeds.\nExtrapolation\nLoop Speed Discussion (no Extrapolation)\nMy ToF returns new data every 108.21 ms on average which corresponds to a rate of 9.24 Hz. This large delay leads to the derivative issue mentioned above. When the code is sped up to eleminate waiting for a new ToF reading, the speed of the loop is now 172.27 Hz.\nExtrapolation Implementation\nThe following extrapolation code was added to the runPID() function. Essentially, two distance arrays are used: tof_time[i] for ToF readings as they are ready and interpolated[counter] which stores interpolated data. Two time arrays are also implemented: tof_time[i] to store time intervals that the sensors collect data at (important for calculating slope) and times[counter] for constant running time (also used by the interpolator).\n\nProportional Control with Extrapolation\nI started with testing extrapolation with proportional control. After many many hours of debugging it finally worked (yielding the above extrapolation code)!\nHere is a run with Kp set to 0.1\n\nProportional Control - Extrapolation\n\nProportional Control - Extrapolation\nWhile the robot does eventually finish at the 1 ft target, it initially overshoots and has to backtrack. This is solved via implementing derivative control to help slow the robot down as the error decreases. When graphing the extrapolated data it follows the expected curve however would benefit from some low pass filtering in the future.\nPD Control with Extrapolation\nHere is the first test after addings the derivative term with extrapolation (PD control). Kp set to 0.1 and Kd set to 3\n\nPD Control #1 - Extrapolation\n\nPD Control #1 - Extrapolation\nWhile the robot does reach the final 1ft target, it slows down a little earlier than it should. To account for this, the Kp term was increased to 0.2. For this final test I also tested to see how far I could back up the car and still have it reach the target distance. Here is the final video with Kp = 0.2 and the car started roughly 2.7m from the wall.\n\nPD Control #2 - Extrapolation\n\nPD Control #2 - Extrapolation\nIn conclusion, when comparing the extrapolated and non-extrapolated results, we can see the car far more able to slow down accurately. This allows for a overall faster speed without overshooting the target and crashing. With near zero steady state error, I did not find it necessary to implement integral control at the moment. However, the infastructure is in the code; if I wanted to use it I would just have to pass through an integral gain and tune!\nCollaboration\nI collaborated extensively on this project with Jack Long and Trevor Dales. I referenced Wenyi’s site for PID implementation code and Daria’s site for help with extrapolation. ChatGPT was used to help plot graphs.\n","path":null},{"url":"https://correial.github.io/LuccaFastRobots/Fast Robots Stuff/lab-4/","title":"Lab 4: Motor Drivers and Open Loop Control","description":null,"body":"Prelab\nSystem Wiring\nI decided to use pins 13, A14, A15, A16 for control on the Artemis. This pin proximity does increase risk of shorting connections, however it allows for a more compact design and greater area of the board to be used for mounting to the car rather than having to avoid largely seperated pins. Furthermore, according to the data sheet pins marked with a (~) indicate PWM capability which is required to send signals to the motor drivers.\n\nArtemis Schematic\nThe motor driver electrical schematic used on my car is shown below:\n\nDrivetrain System Wiring Schematic\nBattery Discussion\nThe Artemis and the motor drivers/motors are powered by separate batteries to help ensure operational stability, protect components, reduce noise, and increase battery lifetime. The two car motors can act as a disruptive inductive load, potentially creating large amounts of noise and posing a damage risk to sensitive parts. Thus powering the Artemis by a second battery allows the motors to run for longer and protect the Artemis circuit (all connected by signal wires) from the motor interferance and inductive load.\nLab Tasks\nPower Supply and Oscilloscope Hookup\nOne motor driver was tested at a time by connecting the inner two OUT pins to the positive scope probe and the outer two OUT pins to the ground scope probe. I also connected VIN and GND to the power supply to power the driver. The power supply is set to 3.7V to simulate the 3.7V that would be supplied by the 850mAh motor battery. Below is a picture of the setup (the power supply and oscilloscope probing wires are drawn in to show the setup used to obtain the PWM signal before I soldered the parts to the car):\n\nScope and Power Supply Connection to Motor Driver\nThe following code was used to visualize the motor driver output:\n\nThe analogWrite(AB1IN_LEFT,100) line indicated a 40% (100/255) duty cycle to pin 16. This was captured on the oscilloscope:\n\nOscilloscope PWM Output (40% Duty)\nMotor Testing\nThe above code used to see the PWM signals on the scope were also used to run one side of wheels:\n\nOne Side Test\nAfter confirming that both motor drivers successfully powered the motors, I tried running the car all together using the 850 mAh battery using the following code:\n\n\nBoth Wheels Spinning (with battery)\nComplete Hardware Integration on Car\nBelow are images of the car after mounting all components. Holes were drilled and zip ties were used to strap down the IMU and both motor drivers at the front. Zip ties were also used to bundle the Artemis and 750mAh battery as well as for wire managment. Double sided tape was used to mount the two ToF sensors (one on the side and one at the front) and the Artemis/battery module at the rear.\n\nTop of Car\n\n    \n        \n        Bottom of Car\n    \n    \n        \n        Battery Connection on Bottom\n    \n\n\nFront of Car\nLower PWM Limit\nI found that the minimum PWM value for which the robot needs to move forward, backward, and on axis by starting with a arbitrary small value and increasing it by 5 until the car no longer stalled. I found that on a fully charged battery the forward and backward lower limit PWM value is 35 and 110 for spinning on axis.\nHere is a video of testing using those PWM values to go forward, in reverse, then spin on axis in both directions:\n\nLower PWM Limit\nOpen Loop Testing and Calibration\nWhen first testing the car’s straightline preformance, it would instantly begin to veer to the right. I also noticed that when traveling in reverse the car had the same issue where the right side lacked power. To account for this I added a calibration factor to increase the PWM signal for the right set of wheels. Initially I went about this by simply adding a calibration factor to the PWM rather than scaling it via a multiplicative calibration factor. While I was able to get the car fairly straight doing this, it only really worked for a set speed. Given this, I decided to switch to a scaling calibration factor which you can see implemented in the code below. I started both scaling factors at 1 and increased them by .05 until I recieved steady straightline results (visible in the Open Loop Test video).\n\n\nspeedLine is the stright line speed for forward and reverse\nspeedTurn is the turning speed\ncorrection_f is the calibration factor for forward wheel rotation (currently 1.05)\ncorrection_b is the calibration factor for reverse wheel rotation (currently 1.3)\n\nTo test a range of car capabilities, I had the car move straight, then in reverse, then rotate on axis in both directions before moving straight left at the end (note each tile is 13“x13“). The sequence of actions is set in the main loop() which gets called when the START_DATA_COLLECTION command flag is run over bluetooth.\n\nOpen Loop Test\nCollaboration\nI collaborated extensively on this project with Jack Long and Trevor Dales. I referenced Wenyi’s site for my wiring setup, initial motor testing code, and general outline for creating funtions to drive forward, spin right, etc. Special thanks to all TA’s that helped debug my electronics and car for hours before we realized I probably needed a new one!\n","path":null},{"url":"https://correial.github.io/LuccaFastRobots/Fast Robots Stuff/lab-3/","title":"Lab 3: Time of Flight Sensors","description":null,"body":"I2C Address and Time-of-Flight Sensor Discussion\nTwo time of flight sensors will used and ultimately mounted on the car to provide different points of view to help the robot naviate; one of my ToF sensors will be mounted on the front center of the car and the other between the two wheels on one side. This will allow me to keep a set distance from walls to the side as well as avoid obstacles and navigate with vision at the front.\nSince we want to use two ToF flight sensors which have the same I2C address, we use the XSHUT pin on one of the sensors to shut off one ToF which modifying the I2C address of the other. It can then be restarted again to function simultaniously with the other. Below is the code to execute this initialization:\n\nPer the data ToF data sheet, the default I2C address is 0x52 (0b 0101 0010).\nUsing the Example05_Wire_I2C.ino example sketch, I was able to scan for connected I2C devices:\n\nI2C Device Scanning\nFrom the scan we can see the Tof device address as 0x29 (0b 0010 1001). Since the least significant bit is used to indicate read/write, it is soley a matter of shifting the 7-bit 0x29 address left by one bit to make space for the read/write bit that appears in the data sheet. Thus, the scanned address matches the data sheet as it is just a shift left away from matching the 0x52 8-bit address.\nPhysical Connection\nIn the below images you can see the battery leads soldered to the QWICC connect cables and the ToF sensors wired as outlined in the schematic:\n\nWiring Schematic\n\nToF sensor connected to QWIIC breakout board\n\nSoldered Battery Pack\nTwo ToF Sensors In Parallel Test\nFrom the following video we can see the two ToF sensors working in parallel. I noticed that when the clearance for my ToF sensor drops below 20 mm, they tend to output 0 mm. However, this is to be expected since according to section 3.3 of the sensor data sheet, the minimum ranging distance is 4 cm. Overall, the sensors are accurate outside of this range.\nI decided to use the long ToF mode distanceSensor1.setDistanceModeLong(). According to the data sheet: “long distance mode allows the longest possible ranging distance of 4 m to be reached.” In a classroom environment I believe that this longer distance will prove more useful for mapping even though it is at the cost of higher resolution at shorter distances.\n\nTwo ToF Sensor Testing\nToF Sensor Speed\nI tested the speed of my ToF sensors in two ways. The first was by printing the Artemis clock to the Serial as fast as possible and printing new ToF sensor data from both sensors only when available. The second was by testing the speed when called over bluetooth and compare it to the IMU.\n\nI used the following function to visualize and quantify the gap between each ToF data collection:\n\n\n\n    \n        \n        Speed Test Frame 1\n    \n    \n        \n        Speed Test Frame 2\n    \n    \n        \n        Speed Test Frame 3\n    \n\nFrom the above three images you can see that the average gap between ToF sensor collection is 90 ms, corresponding to a rate of 11 Hz. Both sensor 1 and sensor two print at roughly the same rate.\n\nI created a function collectTOF() to collect my ToF data in order to ensure the code doesn’t hang while it waits for the sensor to finish a measurement. It is called from the main loop when one of the two ToF sensors are ready AND my start flag is true. The function populates a time and two ToF arrays with data. The data is parsed through via case GET_TOF_READINGS to be sent over bluetooth.\n\n\nAfter collecting data for 5 seconds (using my START_DATA_COLLECTION and STOP_DATA_COLLECTION command flags) the following number data points were collected from the IMU (function shown in Lab 2) and ToF sensors relative to the number of cycles through my main loop. This method helps eliminate the delay from print statments in the first speed test:\n\nIMU and ToF Counters\nFrom the counters we can see that the ToF sensors were considerably slower than the IMU at recieving data and thus would be the limiting factor. After 5 seconds of data collection this corresponds 10 Hz for the ToF and 98 Hz for the IMU.\nToF Sensor Data (Over Bluetooth)\nHere I created a Time vs. Distance graph of ToF data collected on the Artemis then sent over bluetooth:\n\nDistance Testing via Bluetooth\nHere is another graph where I simultaniously collected ToF and IMU data for 5 seconds over bluetooth and graphed them on the same time axis. The complementary filter from Lab 2 was used for the IMU data:\n\nAngle and Distance Collection\nToF Accuracy\nTo quantify the accuracy of the ToF sensors I collected 10 seconds of data at three distances and graphed the data vs. time as well as the set target distance:\n\n100 mm Test\n\n150 mm Test\n\n200 mm Test\nAll around from the graphs you can see that the time of flight sensors are fairly accurate at a range of distances. While the testing setup was fairly precise, a few milimeters of variance is likely due to the human error in trying to perfectly align the placement of the sensors along my ruler. Nonetheless there is certainly some higher frequency noise in the ToF sensors which could likely be reduced by a LPF.\nCollaboration\nI collaborated extensively on this project with Jack Long and Trevor Dales. I referenced Wenyi’s site for my wiring setup, ToF sensor initiation, and sketch used to  print out distance sensors in my serial monitor to test speed. ChatGPT was used to help plot CSV data and format graphs.\n","path":null},{"url":"https://correial.github.io/LuccaFastRobots/Fast Robots Stuff/lab-2/","title":"Lab 2: IMU","description":null,"body":"IMU Setup\nAD0_VAL &amp; Initial Data Observations\nThe AD0_VAL represents the last bit of the I2C address. In our case, 1 is set as the default and that shouldn’t be changed unless the ADR on the board is closed via solder and then should be set to 0.\nAfter testing with the example code as well as the lecture 4 code, it appears that the accelerometer and gyroscope data print as expected. Three axis are printed for both sensor as well as the corresponding unit (mg for acceleration and DPF for the gyroscope). As discussed in lecture, in the data you can see accelerations and rotations being tracked but not absolute position; changes in the printed values only occur with movement. Additionally, you can see that the changing values are dependent on the axis being rotated about and the sensor being observed. When rotating around the z-axis you can see that the accelerometer data does not change, however the gyroscope does. When accelerating the board along the x-axis you can see a change in value for the accelerometer x-axis but not the gyroscope.\n\nArtemis and IMU Connection with BLUE LED Indicator\nAccelerometer\nAccelerometer Data to Pitch and Roll Conversion\n\n\nVideo of IMU Testing - full screen to reduce blur\n\n    \n        \n        0°\n    \n    \n        \n        Pitch @ -90°\n    \n    \n        \n        Pitch @ 90°\n    \n    \n        \n        Roll @ -90°\n    \n    \n        \n        Roll @ 90°\n    \n\nAs you can see from the frozen frames, the accelerometer is very accurate with vary little variation in angle from the expected value. As a result I do not think it is nessesary to do a two-point calibration.\nData Collection and Plotting Code\nThe following code was used to collect the data in arrays and then use Juypter to pipe the data from the Artemis into a CSV file and graph. Note that as I added more arrays to store more data (LPF, Gyro, Complementary Filter) I simply added more columns to the csv via the notification handler. The graphs will be shown later in the lab for analysis.\nArtemis Aruino Code:\n\nJupyter Code:\n\nFourier Transform and Low Pass Filter Plotting\nThe raw data shows some noise in the higher frequencies however it is fairly negligable. This is due to the fact that the IMU has a low pass filter implemented already. Regardless, I will add a low pass filter\nWhen collecting dat in the proximity of the running car, the most noise appeared to be in the range of 0 Hz and 5 Hz; I will make the cutoff at 5 Hz. The lowpass filter effects the output by limiting faster frequencies (which we are defining as noise) from being shown in the data. If the frequency chosen is too small, you will still have unwanted noise in the smaller frequency range. However, if you pick too high of a cutoff frequency you run the risk of ignoring data points that may actually be important and a correct reflection of the robot’s movement (maybe a sharp turn on a flip).\nIn order to apply a low pass filter I had to calculate my alpha value as 0.0876 using following equations:\n$$\\alpha = \\frac{T}{T + RC}$$\n$$ f_c = \\frac{1}{2\\pi RC} $$\n\nT = sampling rate\n$f_c$ = cutoff frequency\n\n\nRaw and Fourier Transform Data with Low Pass Filter - Car in Proximity\n \n\nRaw and Fourier Transform Data with Low Pass Filter - Hand Osscilations\n \n\nRaw and Fourier Transform Data with Low Pass Filter - Hitting Table\nYou can see that the low pass filter is successful in reducing unwanted noise in the accelerometer data. This is especially clear in the final graphic (Raw and Fourier Transform Data with Low Pass Filter - Hand Osscilations) where the LPF works to ignore the spikes in magnitude that comes from hitting the table.\nGyroscope\nEquations to compute pitch, roll, and yaw angles from the gyroscope:\n\nGyroscope vs. Accelerometer Data\nWhen first collecting the gyroscope readings I noticed that the data did not match the accelerometer data. I realized that due to the default axis of the gyroscope, the pitch and roll for the gyroscope really corresponded to the roll and pitch of the accelerometer respectively (and make the pitch negative).\n\nInitial Gyro vs. Accelerometer Readings (Flipping Needed) \nAfter making those changes, I noticed that there was still drift from the gyroscope over time, likely due from integrating the error in each step. However, I did find it interesting that the gyroscop provided cleaner and smoother data during quick direction changed (going from -90 to 90 degrees and back). Thus while the gyroscope alone may not be highly accurate, it is still stable.\n\nRaw Gyro Data vs. Accelerometer Readings \nTo observe the effects of changing the sampling rate, I added delays in my case GET_ACC_READINGS command code FOR loop to slow down the data collection. I noticed that a delay of 10 ms added some choppiness to the plotting without a significant increase it collection time. However, adding a 100 ms delay significantly increased the data collection time as well as the choppiness in the plot. The gyroscope, which I found was especially good at tracking quick changes of direction smoothly is now not nearly as clear. Additionally, you can see the plot jumping around for smoother IMU movements as the gaps between time intervals is increased.\nComplementary Filter Implementation\nThe following code was used to imlpement my complementary filter:\n\n\nComplementary Filter Gyro vs. Low Pass Filter Accelerometer\nFrom the results you can that with an alpha value of 0.0876 the combined measurements from the accelerometer and gyroscope significantly increases stability (which comes from the gyroscope) and accuracy (from the low pass filter accelerometer).\nSampling Data\nSpeed Up\nI took a few measures to speed up the execution time for my main loop:\n\nRemoved the part in my code where I wait for IMU data to be ready (for example checking(myICM.dataReady()) to move through the command loop. Instead I check if data is ready in the main loop and if it is I call the function collectIMU() to compute the pitch, roll and yaw. After computing I add them to their respective arrays and iterate through those arrays with a different command (that does not affect the resolution as data is already collected).\nRemoved debugging print statments in my command to get IMU data\nI use flags to start/stop data recording\n\nWhile my IMU was able to sample new values farily quickly (~ 350 Hz) after cleaning up my code, the main loop runs significantly faster than my IMU produces new data. This is evident when comparing the IMU_Count variable (which only runs when data is collected) to the Total_Loops (which counts the number of times cycled through the main). The Total_Loops is larger on the magnitude of 10-100x which means that the IMU is the holdup.\nCode of main loop function:\n\ncollectIMU() function:\n\nJupyter code to start/stop data collection via setting a global start variable to 1 or 0 within the START_DATA_COLLECTION and START_DATA_COLLECTION commands:\n\nThe old case GET_ACC_READINGS command was called in Jupyter after stopping data collection to then re-popoulate the csv file with new values.\n\nCSV proving population of time-stamped IMU data in arrays\nData Storage\nI decided that it would be best to have seperate arrays for storing accelerometer and gyroscope data rather than one large one. This was partially because I decided that it would be easier to organize and parse through the data using different arrays to compartmentalize the data before sending them over bluetooth. I also found it easier to create a CSV from the seperate arrays in Jupyter.\nEach of these arrays contain floats as the gyroscope and acceleration naturally output decimal values. With a double data type being twice the size of a float (64 vs 32 bits), I decided that a float was the best data type for these sensor arrays.\nI have a total of 10 floats arrays for a total of 40 bytes at a time:\n\n1 for time\n2 for accelerometer roll and pitch\n2 for LPF roll and pitch\n3 for gyroscope roll, pitch, and yaw\n2 for complementary filter data\n\nIn lab 1b global variables use 30,648 bytes. This lab we added the above arrays to send IMU data. If the Artemis board has 384 kB of RAM, then 353,352 bytes of dynamic memor remain which allow us to store 353,352/40 = 8833 data points. With an average step time of 2.86 ms (shown below) we get a sample rate of 349.65 Hz. This corresponds to 25.26 seconds of IMU data collection.\n5 Seconds of IMU Data\n\nProving 5 Seconds of IMU Data\nI used one of my CSV files as an example of collecting at least 5 seconds of data and sending it over bluetooth. To do this I took the difference between the first time stamp and the last time stamp in my proximityFinal.csv file:\n\nRC Stunts\n\nStunt 1\n\nStunt 2\nThe car is quick at direction changing and accelerating. When spinning it is able to hold its position on the ground without drifting much. Note that the car’s speed can not be changed while moving, it can only stop and change directions.\nCollaboration\nI collaborated extensively on this project with Jack Long and Trevor Dales. I referenced Daria’s site for code debugging in my complementary filter as well as visually understanding how to effectively display my plots. ChatGPT was heavily used to write plotting code for the Raw, FFT and LPF data. It also helped me write my FFT function as the provided link had some syntax error and missing pictures.\n","path":null},{"url":"https://correial.github.io/LuccaFastRobots/Fast Robots Stuff/lab-1/","title":"Lab 1: Artemis Setup and Communication","description":null,"body":"Lab 1a\nDuring section 1a of the lab I installed the Arduino IDE and established a wired connection to communicate with the Artemis Nano. To connect I had to select the correct board and port in the Arduino IDE. Then, to test the connection and explore the Arduino environment I completed the following assigned example sketches in the Arduino IDE:\n\nBasics_blink\nApollo3_serial\nApollo3_analogRead\nPDM_microphoneOutput\n\nBlink\nYou can see the Artemis board flash a bright blue led\n\nBlink test video\nSerial\nHere we can see the Artemis recieves the string and echos it back\n\nSerial output test\nanalogRead Temperature Sensor\n\nTemperature sensor test\nMicrophone Output\nFrom the video you can see the Artemis microphone successfully picking up the difference in sounds in the serial monitor\n\nMicrphone output test\nLab 1b\nCodebase and BLE\nBluetooth (specifically Bluetooth LE) at a high level is used to establish a connection between my computer and the Artemis:\n\n\nBluetooth LE radio acts like a community bulletin board where Computers (community members) can connect to read the board. If the radio is a bulletin board we call that a peripheral device (the Artemis in this case) and it is responsible for posting data. If the radio is a reader (central device) it reads from any of the bulletin boards. Essentially, central devices view the services, get the data, then move on, all within a few milliseconds allowing multiple central devices can get data from one peripheral.\n\n\nServices are identified by unique numbers known as UUIDs. The ability to define services and characteristics depends on the radio you’re using and its firmware. Bluetooth LE peripherals will provide services, which in turn provide characteristics.\n\n\nThere are four things that a central device can do with a characteristic: Read, Write, Indicate, and Notify\n\n\nThe codebase is collection of source code files that make up our system. Some important components:\n\n\nThe Artemis’ unique UUID and Mac address allow for undisrupted communication and transmission of BLExCharacteristics\n\n\nble_arduino.ino is the code running on the Artemis, edited in the Arduino IDE\n\n\nEString is used when transmitting strings from the Artemis to your computer\n\n\nRobotCommand.h is used when handling a robot command that the Artemis receives and is of the string format &lt;cmd_type&gt;:&lt;value1&gt; &lt;value2&gt;|&lt;value3&gt;|…\n\n\nDemo.ipynb is where you find the python code sending commands to the Artemis\n\n\nSome relevant functions used to communicate between the computer and Artemis:\n\nsend_command(cmd_type, data) to send a command\nble.connect() and ble.disconnect to connect with the Artemis\nreceive_string(uuid) to recieve a string from our board\nstart_notify(uuid, notification_handler) to activate the notification handler\nble.bytearray_to_string(byteArray) to convert recieved data into string\n\n\n\nConfigurations and Setup\n\n\nI started with installing venv: python3 -m pip install --user virtualenv\n\n\nI created the “FastRobots_ble” virtual environment inside my project directory: python3 -m venv FastRobots_ble\n\n\nI activated the virtual environment: source FastRobots_ble/bin/activate\n\n\nI downloaded the provided ble_robot_1.2 codebase into my project directory\n\n\nIt is now time to start the Jupyter server: jupyter lab\n\n\nUpdated the Artemis MAC Address on the Computer. Run the ble_arduino.ino file in the Arduino IDE and check the serial monitor the MAC address:\n\n\n\nMAC Address\n\nGenerate new UUID: run from uuid import uuid4 and uuid4(). Input the generated UUID into the #define BLE_UUID_TEST_SERVICE line in ble_arduino.ino and into the ble_service: line in connections.yaml\n\n\nconnections.yaml\n\nble_arduino.ino\n\nConnect to the Artemis Nano via BLE\n\n\n\nSuccessful BLE Connection\nTask 1\nI sent a string value from my computer to the Artemis board using the  ECHO command and the computer recieved and printed the augmented string\nArduino Code:\n\nJupyter Lab Code:\n\n\n\nECHO Output\nTask 2\nI sent three floats to the Artemis board using the SEND_THREE_FLOATS command and extracted the three floats in the Arduino sketch\nArduino Code:\n\nJupyter Lab Code:\n\n\nSEND_THREE_FLOATS Output\nTask 3\nI added a GET_TIME_MILLIS which makes the robot reply write a string to the string characteristic. GET_TIME_MILLIS had to be added to the cmd_types.py file. GET_TIME_MILLIS had to be added to cmd_types.py to run. Note that the output looks the same as in task 4.\n\nTask 4\nI setup a notification_handler function to receive the string value from the Artemis board and, in the callback function, extract the time from the string.\n\n\n\n\nNotification Handler Output\nTask 5\nI made a twenty-five step loop that gets the current time in milliseconds using the GET_TIME_MILLIS function to then be processed by notification_handler(). From my output shown below you can see that there was an average 33.5 ms gap between the prints. This translates to 29.85 message transmissions per second. With each message being 9 bytes (1 char per each string sent), this results an effective data transfer rate of 269 bytes per second for this method.\n\n\nGET_TIME_MILLIS Loop Output\nTask 6\nI created a command SEND_TIME_DATA that loops though to add generated time steps via the millis() function and then stores them in an array. Then, in SEND_TIME_DATA I loop through the array and send each data point as a string to my laptop to be processed. SEND_TIME_DATA had to be added to the cmd_types.py file. Note that millisArray[i] is defined as a global array.\nArduino Code\n\nJupyter Code:\n\n\nSEND_TIME_DATA Output\nTask 7\nI created a second array to store fahrenheit temperature readings with the same length as the one used in task 6. Each index in both global arrays (millisArray[] and tempArray[]) correspond to each other. The command GET_TEMP_READINGS loops through both arrays concurrently and sends each temperature reading with a time stamp. The notification handler parses these strings and populates the data into two lists. Note that GET_TEMP_READINGS had to be added to the cmd_types.py file.\n\n\nTemp and Time Output\nTask 8\nRate wise, it is clear from the time steps in tasks 5 vs. 6 that method one is considerably slower at recording data than method two. This is because method one has to wait until the Artemis sends data to the computer after every round of collection before recording again. Instead, the second method can effectively record data as fast as its slowest sensor, thus producing data that may be more accurate but at the expense of a delayed reception on the client’s end. This could result in a slower response time from the robot and thus is less applicable if the robot needs to make time-sensitive decisions from sensor data. For an open-loop test where we do not care as much about real-time feedback, method two may be more useful as the faster data recording would provide higher resolution.\nIn order to determine how quickly the second method records data, I had to increase the number of loop iterations to 100 in order to see a difference in time steps. The first element is T: 104510ms and the 100th is T: 104512ms which translates to data being recorded every 0.02ms on average (considerably faster than the 33.5 ms gap in task 5).\nThe millis() int variable and getTempDegF() int variable are both stored as ints of 4 bytes each for a total of 8 bytes. As printed by the Arduino IDE output, global variables use 30648 bytes. If the Artemis board has 384 kB of RAM, then 353,352 bytes remain allowing us to store a total of 353,352 bytes/8 bytes = 44,169 data points without running out of memory.\nDiscussion and Conclusion (Lab 1A &amp; 1B)\n\nLearned about what functions are responsible for communication between my computer and Artemis and how the commands (ECHO, GET_TIME_MILLIS, etc.) are passed in via RobotCommand.h\nAt first I was confused about the relationship between different data types and their byte size. However, the later questions in the lab made it clear how ints vs. strings require different number of bytes as well as how Estring char are used to send those types to the computer\nThe largest problem I faced was understanding the parameters needed for the notification handler!\n\nCollaboration\nI collaborated extensively on this project with Jack Long and Trevor Dales. I referenced Daria’s site for code debugging and specific help with SEND_TIME_DATA and GET_TEMP_READINGS. ChatGPT was used for Lab 1B code debugging and website formatting/development.\n","path":null}]